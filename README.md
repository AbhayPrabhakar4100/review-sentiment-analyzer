# product-review-sentiment-ml

## ğŸš€ Overview
This project applies **machine learning techniques** to classify product reviews by sentiment.  
The objective is to predict whether a review is **positive** or **negative** using only its text content, and then compare predictions against the actual product ratings.  

By exploring multiple models, this project evaluates trade-offs in accuracy, precision, and recall to identify the most reliable classifier for sentiment prediction.

---

## ğŸ—ƒ Dataset
- **File:** `product_reviews_with_sentiment.csv`  
- **Total Samples:** 34,627  
- **Columns:**  
  - `product_id` â†’ product identifier (dropped)  
  - `product_rating` â†’ rating on a 1â€“5 scale  
  - `product_review` â†’ raw review text (main input feature)  
  - `Unnamed: 0` â†’ index column (removed during preprocessing)  

---

## ğŸ“ Preprocessing
- Removed unused columns (`Unnamed: 0`, `product_id`)  
- Filtered out neutral reviews (rating = 3)  
- Mapped ratings:  
  - Ratings 4 & 5 â†’ **Positive (1)**  
  - Ratings 1 & 2 â†’ **Negative (0)**  
- Addressed class imbalance using **SMOTE** (Synthetic Minority Over-sampling Technique)  

---

## ğŸ”§ Methodology

### 1. Text Vectorization
- TF-IDF (max features: 5000)  
- English stop words removed  

### 2. Class Balancing
- Applied **SMOTE** on training data to equalize positive vs negative samples  

### 3. Models Trained
- Logistic Regression (SMOTE)  
- Random Forest (SMOTE)  
- XGBoost (SMOTE)  

---

## ğŸ“Š Results

| Model                | Accuracy | Recall (Neg) | Precision (Neg) | F1-Score (Neg) |
|----------------------|----------|--------------|-----------------|----------------|
| Logistic Regression  | **92.76%** | 69%         | 22%             | 33%            |
| Random Forest        | **97.34%** | 18%         | 48%             | 27%            |
| XGBoost              | **95.65%** | 48%         | 30%             | 37%            |

â¡ï¸ **Final Choice:** **XGBoost (SMOTE)**  
- Best trade-off between recall and precision for negative class  
- Outperforms Random Forest in recall and achieves stronger balance than Logistic Regression  

---

## ğŸ›  Libraries
- Python 3.x  
- NumPy, Pandas, Scikit-learn  
- imbalanced-learn (SMOTE)  
- XGBoost  

---

## ğŸ“„ License
This project was developed for academic purposes.  
Feel free to **star â­ or fork ğŸ´** if you find it useful!
